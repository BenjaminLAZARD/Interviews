{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCD4dLuVzsWRilTOsxPxVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjaminLAZARD/Interviews/blob/main/Upflowy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the attached image I want you to create a jupyter notebook that you will send through as the test outcome. You will need to do the following:\n",
        "1. identify a hugging-face model that can process the image to text\n",
        "2. The goal of the image to text is to identify the title, body text and button text of the image and return it labelled correctly with text values returned. eg {\n",
        "    title: \"The title text\",\n",
        "    body: \"the body text\",\n",
        "    button: \"the button text\"\n",
        "}\n",
        "3. import a pre-trained version of the model into your notebook\n",
        "4. import the provided image and make it ready for prediction with the model\n",
        "5. run the prediction and output the results\n",
        "\n",
        "The aim of this exercise is to demonstrate your skills in assessing and using a pre-trained model on the type of subject matter we will be using. While the accuracy of the result is not part of the assessment as we are not expecting you to fine-tune the pre-trained model, your ability to choose a model that demonstrates it has the potential to achieve the accuracy with fine tuning will be considered.\n",
        "\n",
        "I need you to time-box the work on this to not more than 2 hours, so you need to think carefully about how you allocate your time to the different parts of this task. Please make small regular commits to a git repository to demonstrate your work methodology and the time taken. If you don't complete the task in the 2 hours, please submit what you have achieved.\n"
      ],
      "metadata": {
        "id": "xkjim_zCZ9ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.\n",
        "\n",
        "Identifying some huggingface models that can process image to text and label correctly each text as per the document. Or identify strategies to reach that goal.\n",
        "\n",
        "## 1.1 Finding a model specifically designed for such labelization\n",
        "\n",
        "## 1.2 Finding a model that can do OCR and return metadata\n",
        "If we can retrieve not only the text, but also elements like fontsize and boundingboxes, we can use hand-made rules to classify the texts (for example the bigger font and shortest text can be a title, or a button will have a different background color than the rest).\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.19.2/en/model_doc/layoutlmv2 -> seems to be returning bounding boxes, question answering can be attempted to ask for the different output json parts.\n",
        "\n",
        "Did not find such a model\n",
        "[Donut model](https://huggingface.co/docs/transformers/main/en/model_doc/donut#inference-examples) is somewhat close.\n",
        "\n",
        "## 1.3 Finding a model that can do only OCR but still separate texts detected, then use traditional NLP techniques\n",
        "A button will contain few words for example, likely be a verb or start by one. Maybe there exist some models to claissfy content in some way we could use.\n",
        "\n",
        "This is not huggingface, but [this tutorial](https://nanonets.com/blog/ocr-with-tesseract/) mentions the use of Google's Tesseract and includes a hack for visualization of bounding boxes around detected text. This can be a great if we don't plan to finetune the OCR part.\n",
        "\n",
        "There exist the pipeline(\"image-to-text\", model= <>) tool to use TOCR models (but it is designed mostly for single line texts).\n",
        "\n",
        "## 1.4 Preprocessing the image manually\n",
        "Instead of directly applying a huggingface model, one could actually use a library like opencv2, or the base of a HF architecture to simply split the image into different text areas, then run the OCR on each independently (for example as in [this tutorial](https://huggingface.co/vanshp123/ocrmnist) )\n",
        "\n",
        "\n",
        "## Thinking further\n",
        "\n",
        "Although this is out of the scope of this exercise, it would be interesting to combine approach from [1.2]() together with a classifier that could be trained in-house with the three categories (or more) [title, body, button].\n",
        "\n",
        "For training, the logic mentionned in [this tutorial](https://www.youtube.com/watch?v=aGcLSH9TTLU) could be used\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iDFrKicLZr-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRQLdjL1pGqD",
        "outputId": "c57397d3-777d-4b9a-edbe-4b805b920034"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 9 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,475 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying to move forward with layoutLMV2\n",
        "\n",
        "from transformers import LayoutLMv2Processor\n",
        "from PIL import Image\n",
        "\n",
        "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
        "\n",
        "image = Image.open(\"sample_image.png\").convert(\"RGB\")\n",
        "encoding = processor(image, return_tensors=\"pt\")  # you can also add all tokenizer parameters here such as padding, truncation"
      ],
      "metadata": {
        "id": "EhM9DNKGmWS0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q29j9ri6pBfM",
        "outputId": "d69a3451-0bff-43f1-fe99-8c43852ba376"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.tokenizer.decode(encoding[\"input_ids\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "38TZOOgVqb85",
        "outputId": "3e767803-4889-481e-df1f-c0ad914286be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] take your development to the next level whether you are just getting started on arm or looking for resources to help you create top - performing software solutions, arm developer hub has everything you need to start building better software and deliver rich experiences for billions of devices. download, learn, connect, and question within our growing global developer community. explore arm developer hub [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# So we actually don't get the bounding boxes per section as we wanted..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2C1INtCq_XQ",
        "outputId": "fc8ea226-528d-45f4-9587-d020e0dd0f9e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now trying another option"
      ],
      "metadata": {
        "id": "v_lQFk8XrkdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CsYgAayrkrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tuuJOzLCrkz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmDSSzDnrk3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFeDNo8Trk64"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}