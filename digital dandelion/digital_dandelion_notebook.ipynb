{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8df51c-537e-48c5-a735-af1c8951f590",
   "metadata": {},
   "source": [
    "# End-to-End Solution\n",
    "\n",
    "This notebook is built assuming a GPU environment is available.\n",
    "This is of course just a jupyter demo, but cuda should be enabled.\n",
    "\n",
    "If using a free  jupyter notebook environment, use a T4 GPU environment. You can even [open a terminal now](https://blog.infuseai.io/run-a-full-tty-terminal-in-google-colab-without-colab-pro-2759b9f8a74a)\n",
    "\n",
    "## Dependencies management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1513937d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:33:50.039811Z",
     "iopub.status.busy": "2024-02-29T22:33:50.039418Z",
     "iopub.status.idle": "2024-02-29T22:33:50.672183Z",
     "shell.execute_reply": "2024-02-29T22:33:50.671518Z",
     "shell.execute_reply.started": "2024-02-29T22:33:50.039766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/bin/mamba\n"
     ]
    }
   ],
   "source": [
    "# pick a dependency solver.\n",
    "# here I use saturn cloud (Google Colab GPU ran out on me) and mamba is preinstalled\n",
    "# I usually pick mamba, poetry and uv\n",
    "! which mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339906d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:33:54.046929Z",
     "iopub.status.busy": "2024-02-29T22:33:54.046540Z",
     "iopub.status.idle": "2024-02-29T22:34:24.126777Z",
     "shell.execute_reply": "2024-02-29T22:34:24.125975Z",
     "shell.execute_reply.started": "2024-02-29T22:33:54.046900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "! mamba install -y tensorflow-gpu ffmpeg ffmpeg-python srt pytorch torchvision torchaudio pytorch-cuda>=12 pyaudio -c pytorch -c nvidia -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58dc7e5-54b9-4673-9f84-c1ca77eb895e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:34:24.128448Z",
     "iopub.status.busy": "2024-02-29T22:34:24.128146Z",
     "iopub.status.idle": "2024-02-29T22:34:24.739422Z",
     "shell.execute_reply": "2024-02-29T22:34:24.738788Z",
     "shell.execute_reply.started": "2024-02-29T22:34:24.128422Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Wed_Nov_22_10:17:15_PST_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.107\n",
      "Build cuda_12.3.r12.3/compiler.33567101_0\n"
     ]
    }
   ],
   "source": [
    "# Check that a cuda environment exists now\n",
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1d62b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:34:24.740595Z",
     "iopub.status.busy": "2024-02-29T22:34:24.740301Z",
     "iopub.status.idle": "2024-02-29T22:34:31.067797Z",
     "shell.execute_reply": "2024-02-29T22:34:31.066976Z",
     "shell.execute_reply.started": "2024-02-29T22:34:24.740572Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-lu5nt0yo\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-lu5nt0yo\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numba in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from openai-whisper==20231117) (0.58.1)\n",
      "Requirement already satisfied: numpy in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from openai-whisper==20231117) (1.26.3)\n",
      "Requirement already satisfied: torch in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from openai-whisper==20231117) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from openai-whisper==20231117) (4.66.1)\n",
      "Requirement already satisfied: more-itertools in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from openai-whisper==20231117) (10.2.0)\n",
      "Requirement already satisfied: tiktoken in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from openai-whisper==20231117) (0.6.0)\n",
      "Requirement already satisfied: triton<3,>=2.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from openai-whisper==20231117) (2.1.0)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# some dependencies are harder to find. whisper install only worked through git for me\n",
    "! pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cac8a-ec9a-4602-9e89-3b070fa3baba",
   "metadata": {},
   "source": [
    "## Audio File Transcription\n",
    "\n",
    "As a first stage, let us try to get through whisper and the use of an appropriate external VAD (Silero) to get the transcription of an audio file.\n",
    "Based on this [tutorial](https://colab.research.google.com/github/ANonEntity/WhisperWithVAD/blob/main/WhisperWithVAD.ipynb#scrollTo=sos9vsxPkIN7) where they also use deepl for compatibility with multiple languages. For now we'll assume english for simplicity.\n",
    "\n",
    "Next stage would be to reproduce this result through streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5818d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T23:27:18.041050Z",
     "iopub.status.busy": "2024-02-28T23:27:18.040532Z",
     "iopub.status.idle": "2024-02-28T23:28:03.834685Z",
     "shell.execute_reply": "2024-02-28T23:28:03.833852Z",
     "shell.execute_reply.started": "2024-02-28T23:27:18.041011Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 23:27:18.641731: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 23:27:18.686061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 23:27:18.686091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 23:27:18.687030: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 23:27:18.693985: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding audio...\n",
      "Running VAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to /home/jovyan/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [00:20<00:00, 74.1MiB/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Subs written to transcription_test.srt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"transcription_test.mp3\"\n",
    "model_size = \"medium\"  # [\"medium\", \"large\"]\n",
    "language = \"english\"\n",
    "translation_mode = \"End-to-end Whisper (default)\"  # [\"End-to-end Whisper (default)\", \"Whisper -> DeepL\", \"No translation\"]\n",
    "\n",
    "source_separation = False\n",
    "vad_threshold = 0.4\n",
    "chunk_threshold = 3.0\n",
    "deepl_target_lang = \"EN-US\"\n",
    "max_attempts = 1\n",
    "initial_prompt = \"\"\n",
    "\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import ffmpeg\n",
    "import srt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert max_attempts >= 1\n",
    "assert vad_threshold >= 0.01\n",
    "assert chunk_threshold >= 0.1\n",
    "assert audio_path != \"\"\n",
    "assert language != \"\"\n",
    "\n",
    "\n",
    "task = \"transcribe\"\n",
    "\n",
    "out_path = os.path.splitext(audio_path)[0] + \".srt\"\n",
    "out_path_pre = os.path.splitext(audio_path)[0] + \"_Untranslated.srt\"\n",
    "\n",
    "# if source_separation:\n",
    "#     print(\"Separating vocals...\")\n",
    "#     !ffprobe -i \"{audio_path}\" -show_entries format=duration -v quiet -of csv=\"p=0\" > input_length\n",
    "#     with open(\"input_length\") as f:\n",
    "#         input_length = int(float(f.read())) + 1\n",
    "#     !spleeter separate -d {input_length} -p spleeter:2stems -o output \"{audio_path}\"\n",
    "#     spleeter_dir = os.path.basename(os.path.splitext(audio_path)[0])\n",
    "#     audio_path = \"output/\" + spleeter_dir + \"/vocals.wav\"\n",
    "\n",
    "print(\"Encoding audio...\")\n",
    "if not os.path.exists(\"vad_chunks\"):\n",
    "    os.mkdir(\"vad_chunks\")\n",
    "ffmpeg.input(audio_path).output(\n",
    "    \"vad_chunks/silero_temp.wav\",\n",
    "    ar=\"16000\",\n",
    "    ac=\"1\",\n",
    "    acodec=\"pcm_s16le\",\n",
    "    map_metadata=\"-1\",\n",
    "    fflags=\"+bitexact\",\n",
    ").overwrite_output().run(quiet=True)\n",
    "\n",
    "print(\"Running VAD...\")\n",
    "model, utils = torch.hub.load(\n",
    "    repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", onnx=False\n",
    ")\n",
    "\n",
    "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
    "\n",
    "# Generate VAD timestamps\n",
    "VAD_SR = 16000\n",
    "wav = read_audio(\"vad_chunks/silero_temp.wav\", sampling_rate=VAD_SR)\n",
    "t = get_speech_timestamps(wav, model, sampling_rate=VAD_SR, threshold=vad_threshold)\n",
    "\n",
    "# Add a bit of padding, and remove small gaps\n",
    "for i in range(len(t)):\n",
    "    t[i][\"start\"] = max(0, t[i][\"start\"] - 3200)  # 0.2s head\n",
    "    t[i][\"end\"] = min(wav.shape[0] - 16, t[i][\"end\"] + 20800)  # 1.3s tail\n",
    "    if i > 0 and t[i][\"start\"] < t[i - 1][\"end\"]:\n",
    "        t[i][\"start\"] = t[i - 1][\"end\"]  # Remove overlap\n",
    "\n",
    "# If breaks are longer than chunk_threshold seconds, split into a new audio file\n",
    "# This'll effectively turn long transcriptions into many shorter ones\n",
    "u = [[]]\n",
    "for i in range(len(t)):\n",
    "    if i > 0 and t[i][\"start\"] > t[i - 1][\"end\"] + (chunk_threshold * VAD_SR):\n",
    "        u.append([])\n",
    "    u[-1].append(t[i])\n",
    "\n",
    "# Merge speech chunks\n",
    "for i in range(len(u)):\n",
    "    save_audio(\n",
    "        \"vad_chunks/\" + str(i) + \".wav\",\n",
    "        collect_chunks(u[i], wav),\n",
    "        sampling_rate=VAD_SR,\n",
    "    )\n",
    "\n",
    "os.remove(\"vad_chunks/silero_temp.wav\")\n",
    "\n",
    "# Convert timestamps to seconds\n",
    "for i in range(len(u)):\n",
    "    time = 0.0\n",
    "    offset = 0.0\n",
    "    for j in range(len(u[i])):\n",
    "        u[i][j][\"start\"] /= VAD_SR\n",
    "        u[i][j][\"end\"] /= VAD_SR\n",
    "        u[i][j][\"chunk_start\"] = time\n",
    "        time += u[i][j][\"end\"] - u[i][j][\"start\"]\n",
    "        u[i][j][\"chunk_end\"] = time\n",
    "        if j == 0:\n",
    "            offset += u[i][j][\"start\"]\n",
    "        else:\n",
    "            offset += u[i][j][\"start\"] - u[i][j - 1][\"end\"]\n",
    "        u[i][j][\"offset\"] = offset\n",
    "\n",
    "# Run Whisper on each audio chunk\n",
    "print(\"Running Whisper...\")\n",
    "model = whisper.load_model(model_size)\n",
    "subs = []\n",
    "segment_info = []\n",
    "sub_index = 1\n",
    "suppress_low = []  # words to remove\n",
    "suppress_high = []  # words to remove\n",
    "for i in tqdm(range(len(u))):\n",
    "    line_buffer = []  # Used for DeepL\n",
    "    for x in range(max_attempts):\n",
    "        result = model.transcribe(\n",
    "            \"vad_chunks/\" + str(i) + \".wav\",\n",
    "            task=task,\n",
    "            language=language,\n",
    "            initial_prompt=initial_prompt,\n",
    "        )\n",
    "        # Break if result doesn't end with severe hallucinations\n",
    "        if len(result[\"segments\"]) == 0:\n",
    "            break\n",
    "        elif result[\"segments\"][-1][\"end\"] < u[i][-1][\"chunk_end\"] + 10.0:\n",
    "            break\n",
    "        elif x + 1 < max_attempts:\n",
    "            print(\"Retrying chunk\", i)\n",
    "    for r in result[\"segments\"]:\n",
    "        # Skip audio timestamped after the chunk has ended\n",
    "        if r[\"start\"] > u[i][-1][\"chunk_end\"]:\n",
    "            continue\n",
    "        # Reduce log probability for certain words/phrases\n",
    "        for s in suppress_low:\n",
    "            if s in r[\"text\"]:\n",
    "                r[\"avg_logprob\"] -= 0.15\n",
    "        for s in suppress_high:\n",
    "            if s in r[\"text\"]:\n",
    "                r[\"avg_logprob\"] -= 0.35\n",
    "        # Keep segment info for debugging\n",
    "        del r[\"tokens\"]\n",
    "        segment_info.append(r)\n",
    "        # Skip if log prob is low or no speech prob is high\n",
    "        if r[\"avg_logprob\"] < -1.0 or r[\"no_speech_prob\"] > 0.7:\n",
    "            continue\n",
    "        # Set start timestamp\n",
    "        start = r[\"start\"] + u[i][0][\"offset\"]\n",
    "        for j in range(len(u[i])):\n",
    "            if (\n",
    "                r[\"start\"] >= u[i][j][\"chunk_start\"]\n",
    "                and r[\"start\"] <= u[i][j][\"chunk_end\"]\n",
    "            ):\n",
    "                start = r[\"start\"] + u[i][j][\"offset\"]\n",
    "                break\n",
    "        # Prevent overlapping subs\n",
    "        if len(subs) > 0:\n",
    "            last_end = datetime.timedelta.total_seconds(subs[-1].end)\n",
    "            if last_end > start:\n",
    "                subs[-1].end = datetime.timedelta(seconds=start)\n",
    "        # Set end timestamp\n",
    "        end = u[i][-1][\"end\"] + 0.5\n",
    "        for j in range(len(u[i])):\n",
    "            if r[\"end\"] >= u[i][j][\"chunk_start\"] and r[\"end\"] <= u[i][j][\"chunk_end\"]:\n",
    "                end = r[\"end\"] + u[i][j][\"offset\"]\n",
    "                break\n",
    "        # Add to SRT list\n",
    "        subs.append(\n",
    "            srt.Subtitle(\n",
    "                index=sub_index,\n",
    "                start=datetime.timedelta(seconds=start),\n",
    "                end=datetime.timedelta(seconds=end),\n",
    "                content=r[\"text\"].strip(),\n",
    "            )\n",
    "        )\n",
    "        sub_index += 1\n",
    "\n",
    "with open(\"segment_info.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(segment_info, f, indent=4)\n",
    "\n",
    "# Write SRT file\n",
    "# Removal of garbage lines\n",
    "garbage_list = []\n",
    "need_context_lines = []\n",
    "clean_subs = list()\n",
    "last_line_garbage = False\n",
    "for i in range(len(subs)):\n",
    "    c = subs[i].content\n",
    "    c = (\n",
    "        c.replace(\".\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\":\", \"\")\n",
    "        .replace(\";\", \"\")\n",
    "        .replace(\"!\", \"\")\n",
    "        .replace(\"?\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\"  \", \" \")\n",
    "        .lower()\n",
    "    )\n",
    "    is_garbage = True\n",
    "    for w in c.split(\" \"):\n",
    "        if w.strip() == \"\":\n",
    "            continue\n",
    "        if w.strip() in garbage_list:\n",
    "            continue\n",
    "        elif w.strip() in need_context_lines and last_line_garbage:\n",
    "            continue\n",
    "        else:\n",
    "            is_garbage = False\n",
    "            break\n",
    "    if not is_garbage:\n",
    "        clean_subs.append(subs[i])\n",
    "    last_line_garbage = is_garbage\n",
    "with open(out_path, \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(srt.compose(clean_subs))\n",
    "print(\"\\nDone! Subs written to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b62189-6393-481f-ac58-7e6bba1af208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T23:35:35.617595Z",
     "iopub.status.busy": "2024-02-28T23:35:35.617144Z",
     "iopub.status.idle": "2024-02-28T23:35:36.297044Z",
     "shell.execute_reply": "2024-02-28T23:35:36.296078Z",
     "shell.execute_reply.started": "2024-02-28T23:35:35.617562Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:01,114 --> 00:00:05,114\n",
      "This is a live recording and a test for live transcription.\n",
      "\n",
      "2\n",
      "00:00:05,114 --> 00:00:10,114\n",
      "My name is Benjamin and I'm talking to you, the avatar.\n",
      "\n",
      "3\n",
      "00:00:10,114 --> 00:00:16,114\n",
      "What I want to know is how many people live in Paris in 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! cat transcription_test.srt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b35e72-dc17-4e62-8007-a9ef3ceb8a11",
   "metadata": {},
   "source": [
    " This is a clear success!\n",
    "    \n",
    "Now let us try a similar  technique but from an audio stream\n",
    "\n",
    "## Transcription of a live stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "858f1d4d-f46f-4d51-8086-dde781d1ed08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T23:40:38.744153Z",
     "iopub.status.busy": "2024-02-28T23:40:38.743685Z",
     "iopub.status.idle": "2024-02-28T23:40:39.450493Z",
     "shell.execute_reply": "2024-02-28T23:40:39.448950Z",
     "shell.execute_reply.started": "2024-02-28T23:40:38.744111Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1342:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5727:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM sysdefault\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1342:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5727:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM sysdefault\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.front\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround40\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround41\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround50\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround51\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround71\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1342:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5727:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1342:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5727:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_card_id returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1342:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5727:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2675:(snd_pcm_open_noupdate) Unknown PCM dmix\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -9996] Invalid input device (no default output device)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m p \u001b[38;5;241m=\u001b[39m pyaudio\u001b[38;5;241m.\u001b[39mPyAudio()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Open audio stream\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFORMAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHANNELS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mframes_per_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize Whisper model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pyaudio/__init__.py:639\u001b[0m, in \u001b[0;36mPyAudio.open\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Opens a new stream.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    See constructor for :py:func:`PyAudio.Stream.__init__` for parameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    :returns: A new :py:class:`PyAudio.Stream`\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[43mPyAudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streams\u001b[38;5;241m.\u001b[39madd(stream)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pyaudio/__init__.py:441\u001b[0m, in \u001b[0;36mPyAudio.Stream.__init__\u001b[0;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[1;32m    438\u001b[0m     arguments[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream_callback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stream_callback\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# calling pa.open returns a stream object\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_latency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39minputLatency\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_latency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39moutputLatency\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -9996] Invalid input device (no default output device)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pyaudio\n",
    "import whisper\n",
    "\n",
    "# Define audio stream parameters\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1  # don't need left and right here\n",
    "RATE = 16000  # sampling rate (number of audio samples per second)\n",
    "CHUNK_TIME = 5  # measured in seconds\n",
    "CHUNK = 48000  # number of samples\n",
    "\n",
    "# Create PyAudio object\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Open audio stream\n",
    "stream = p.open(\n",
    "    format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK\n",
    ")\n",
    "\n",
    "# Initialize Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "try:\n",
    "    print(\"Start speaking...\")\n",
    "\n",
    "    while True:\n",
    "        data = stream.read(CHUNK)\n",
    "\n",
    "        # Transcribe audio chunk\n",
    "        result = model.transcribe(audio=data)\n",
    "\n",
    "        # Extract text from result and print it **immediately**\n",
    "        print(result[\"text\"])\n",
    "\n",
    "        # Optionally, clear the transcribed text for the next chunk\n",
    "        # (reduces memory usage but discards previous text)\n",
    "        result[\"text\"] = \"\"\n",
    "\n",
    "        # Exit on user input (optional)\n",
    "        if input(\"Press 'q' to quit: \") == \"q\":\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nExiting...\")\n",
    "\n",
    "finally:\n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    # Close PyAudio\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b447c-4bc1-4f52-a56b-6e3760766877",
   "metadata": {},
   "source": [
    "Given I am executing this notebook in the cloud, my own machine's microphone is not available.\n",
    "Let us skip this part for now\n",
    "\n",
    "## Getting an avatar \n",
    "\n",
    "We are using  a LLM to reply to the user, than a Text-to-speech approach, then the [MakeItTalk paper](https://github.com/yzhou359/MakeItTalk/blob/main/quick_demo.ipynb) here\n",
    "\n",
    "### First we need to generate an answer\n",
    "We can use any LLM here. Using an API can be fast and avoid any infrastructure cost for  this specific need.\n",
    "Yet using a solution like Mistral or llama could be much faster.\n",
    "\n",
    "Here I will generate the answer using Huggingface's API. [Check it out](https://huggingface.co/mistralai/Mistral-7B-v0.1?text=This+is+a+live+recording+and+a+test+for+live+transcription.%0D%0A+My+name+is+Benjamin+and+Im+talking+to+you%2C+the+avatar.%0D%0A+What+I+want+to+know+is+how+many+people+live+in+Paris+in+2023.)\n",
    "\n",
    "For a commercial product, hosting the model would be possible. Specifically this exercise is about a \"select few numbers of users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07af3c8-64a2-4a83-83e1-5a70f2adb76a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:34:33.639386Z",
     "iopub.status.busy": "2024-02-29T22:34:33.638977Z",
     "iopub.status.idle": "2024-02-29T22:34:33.645076Z",
     "shell.execute_reply": "2024-02-29T22:34:33.644247Z",
     "shell.execute_reply.started": "2024-02-29T22:34:33.639354Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a live recording and a test for live transcription.\n",
      " My name is Benjamin and Im talking to you, the avatar.\n",
      " What I want to know is how many people live in Paris in 2023.\n"
     ]
    }
   ],
   "source": [
    "# %cd ../\n",
    "import json\n",
    "\n",
    "with open(\"segment_info.json\", \"r+\") as f:\n",
    "    j = json.load(f)\n",
    "question = \"\\n\".join([e[\"text\"].replace('\"', \"\").replace(\"'\", \"\") for e in j])\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f359b2d-bb9d-4742-9c56-883c82b81f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# # Replace \"YOUR_API_KEY\" with your actual OpenAI API key\n",
    "# openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# response = openai.Completion.create(\n",
    "#     engine=\"text-davinci-003\",  # Choose the appropriate model\n",
    "#     prompt=question,\n",
    "#     max_tokens=100,  # Limit response length (optional)\n",
    "#     temperature=0.5,  # Control creativity (optional)\n",
    "# )\n",
    "\n",
    "## Alternatively\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Install the required libraries:\n",
    "# pip install transformers\n",
    "\n",
    "# Load the Mistral model and tokenizer\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# # Encode the prompt into a format the model understandsé\n",
    "# input_ids = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate text using the model (beam search with 3 beams)\n",
    "# output = model.generate(\n",
    "#     input_ids=input_ids,\n",
    "#     max_length=50,  # Adjust the maximum generated text length\n",
    "#     num_beams=3,\n",
    "# )\n",
    "\n",
    "# # Decode the generated tokens back into text\n",
    "# response = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22333d68-78be-4b2a-a34e-8aee5580c208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:12:22.281588Z",
     "iopub.status.busy": "2024-02-29T22:12:22.281102Z",
     "iopub.status.idle": "2024-02-29T22:12:22.286290Z",
     "shell.execute_reply": "2024-02-29T22:12:22.285391Z",
     "shell.execute_reply.started": "2024-02-29T22:12:22.281556Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assumed response\n",
    "response = \"\"\"\n",
    "Based on the information I have access to. As of January 1, 2023, the estimated population of Paris is:\n",
    "\n",
    "2,102,650 residents (source: statista.com)\n",
    "It's important to note that population data can change over time, so it's always recommended to refer to reliable sources for the most up-to-date information.\n",
    "\n",
    "I hope this information is helpful!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c556d6-148a-4cbe-b83c-c070bd7ef095",
   "metadata": {},
   "source": [
    "### Then we need to create a voice sample based on the text we got out\n",
    "\n",
    "We could use [voice-cloning](https://www.adrianbulat.com/downloads/python-fan) but will stick to Google TTS API for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4253149-e14d-4eb6-a17a-8b7f16207fc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:46:19.625763Z",
     "iopub.status.busy": "2024-02-29T21:46:19.625368Z",
     "iopub.status.idle": "2024-02-29T21:47:01.103405Z",
     "shell.execute_reply": "2024-02-29T21:47:01.102544Z",
     "shell.execute_reply.started": "2024-02-29T21:46:19.625734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for: ['gtts']\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch                                              No change\n",
      "pkgs/main/linux-64                                            No change\n",
      "pkgs/r/linux-64                                               No change\n",
      "[+] 0.1s\n",
      "conda-forge/linux-64 \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "conda-forge/noarch   \u001b[90m━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/noarch                                                 No change\n",
      "[+] 0.2s\n",
      "conda-forge/linux-64 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 564.5kB /  32.8MB @   3.4MB/s  0.2s\n",
      "conda-forge/noarch   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 606.2kB /  13.7MB @   3.6MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
      "conda-forge/linux-64 ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m   7.9MB /  32.8MB @  26.6MB/s  0.3s\n",
      "conda-forge/noarch   ━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━\u001b[0m   7.2MB /  13.7MB @  25.4MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
      "conda-forge/linux-64 ━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━\u001b[0m  14.5MB /  32.8MB @  40.3MB/s  0.4s\n",
      "conda-forge/noarch   ━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━\u001b[0m   7.5MB /  13.7MB @  21.9MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━\u001b[0m  21.4MB /  32.8MB @  45.9MB/s  0.5s\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m  12.0MB /  13.7MB @  26.5MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                  13.7MB @  24.1MB/s  0.6s\n",
      "conda-forge/linux-64                                32.8MB @  55.7MB/s  0.6s\n",
      "\u001b[?25h\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /opt/saturncloud/envs/saturn\n",
      "\n",
      "  Updating specs:\n",
      "\n",
      "   - gtts\n",
      "   - ca-certificates\n",
      "   - certifi\n",
      "   - openssl\n",
      "\n",
      "\n",
      "  Package  Version  Build         Channel         Size\n",
      "────────────────────────────────────────────────────────\n",
      "  Install:\n",
      "────────────────────────────────────────────────────────\n",
      "\n",
      "  \u001b[32m+ gtts \u001b[0m    2.5.1  pyhd8ed1ab_0  conda-forge     35kB\n",
      "\n",
      "  Summary:\n",
      "\n",
      "  Install: 1 packages\n",
      "\n",
      "  Total download: 35kB\n",
      "\n",
      "────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "Downloading      \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B                            0.0s\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
      "Downloading  (1) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B gtts                       0.0s\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
      "Downloading  (1) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B gtts                       0.1s\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Ggtts                                                34.6kB @ 155.8kB/s  0.2s\n",
      "[+] 0.3s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  34.6kB                            0.2s\n",
      "Extracting       ━━━━━━━━━━━━━━━━━━━━━━━       1                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25h\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "! mamba install -y gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b6375bb-179f-49a3-8e16-5129cfe19c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:12:32.696168Z",
     "iopub.status.busy": "2024-02-29T22:12:32.695775Z",
     "iopub.status.idle": "2024-02-29T22:12:35.478803Z",
     "shell.execute_reply": "2024-02-29T22:12:35.478081Z",
     "shell.execute_reply.started": "2024-02-29T22:12:32.696142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "\n",
    "# Define language and speed (optional)\n",
    "tts = gTTS(text=response, lang=\"en\", slow=False)\n",
    "\n",
    "# Save audio file\n",
    "tts.save(\"reply.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a33d0f-e65f-4111-9e1f-180f87c0d869",
   "metadata": {},
   "source": [
    "### Then we need to create the animation and reproduce it\n",
    "Here using [MakeItTalk tutorial](https://github.com/yzhou359/MakeItTalk/blob/main/quick_demo.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f68451d-3983-461c-bb6f-de712be9b9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:34:49.316343Z",
     "iopub.status.busy": "2024-02-29T22:34:49.315968Z",
     "iopub.status.idle": "2024-02-29T22:34:51.870934Z",
     "shell.execute_reply": "2024-02-29T22:34:51.870217Z",
     "shell.execute_reply.started": "2024-02-29T22:34:49.316318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (4.9.0)\n",
      "Requirement already satisfied: face_alignment in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: pydub in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: soundfile in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: librosa==0.9.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: pysptk in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: pyworld in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (0.3.4)\n",
      "Requirement already satisfied: resemblyzer in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: tensorboardX in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: pynormalize in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (5.1.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (0.4.2)\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (0.58.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (1.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from librosa==0.9.1) (23.2)\n",
      "Requirement already satisfied: torch in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from face_alignment) (2.1.0)\n",
      "Requirement already satisfied: scikit-image in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from face_alignment) (0.22.0)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from face_alignment) (4.66.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: cython>=0.28.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pysptk) (3.0.8)\n",
      "Requirement already satisfied: webrtcvad>=2.0.10 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from resemblyzer) (2.0.10)\n",
      "Requirement already satisfied: typing in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from resemblyzer) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from tensorboardX) (4.24.4)\n",
      "Requirement already satisfied: mutagen>=1.40.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pynormalize) (1.47.0)\n",
      "Requirement already satisfied: pycparser in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from numba>=0.45.1->librosa==0.9.1) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1) (4.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1) (2.31.0)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->face_alignment) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->face_alignment) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->face_alignment) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->face_alignment) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->face_alignment) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from torch->face_alignment) (2023.12.2)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from scikit-image->face_alignment) (10.2.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from scikit-image->face_alignment) (2.34.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from scikit-image->face_alignment) (2024.2.12)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from scikit-image->face_alignment) (0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from jinja2->torch->face_alignment) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from sympy->torch->face_alignment) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python face_alignment scikit-learn pydub soundfile librosa==0.9.1 pysptk pyworld resemblyzer tensorboardX pynormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf942a75-6771-467f-8b29-2673fbc274ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:35:07.609196Z",
     "iopub.status.busy": "2024-02-29T22:35:07.608776Z",
     "iopub.status.idle": "2024-02-29T22:35:08.212511Z",
     "shell.execute_reply": "2024-02-29T22:35:08.211656Z",
     "shell.execute_reply.started": "2024-02-29T22:35:07.609167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/yzhou359/MakeItTalk\n",
    "! export PYTHONPATH=MakeItTalk:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e3d2c8-6468-49da-bd5a-bc7ee96b480b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:35:13.737931Z",
     "iopub.status.busy": "2024-02-29T22:35:13.737543Z",
     "iopub.status.idle": "2024-02-29T22:35:46.866582Z",
     "shell.execute_reply": "2024-02-29T22:35:46.865460Z",
     "shell.execute_reply.started": "2024-02-29T22:35:13.737905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘MakeItTalk/examples/dump’: File exists\n",
      "mkdir: cannot create directory ‘MakeItTalk/examples/ckpt’: File exists\n",
      "Requirement already satisfied: gdown in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
      "From (redirected): https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x&confirm=t&uuid=ce64cde6-b090-486e-8436-fced2f9bc42d\n",
      "To: /home/jovyan/workspace/MakeItTalk/examples/ckpt/ckpt_autovc.pth\n",
      "100%|████████████████████████████████████████| 172M/172M [00:01<00:00, 87.9MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
      "To: /home/jovyan/workspace/MakeItTalk/examples/ckpt/ckpt_content_branch.pth\n",
      "100%|██████████████████████████████████████| 7.88M/7.88M [00:00<00:00, 45.0MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
      "To: /home/jovyan/workspace/MakeItTalk/examples/ckpt/ckpt_speaker_branch.pth\n",
      "100%|██████████████████████████████████████| 15.4M/15.4M [00:00<00:00, 60.1MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
      "From (redirected): https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a&confirm=t&uuid=0034bba6-cdbc-4301-bb07-db73555a9f41\n",
      "To: /home/jovyan/workspace/MakeItTalk/examples/ckpt/ckpt_116_i2i_comb.pth\n",
      "100%|█████████████████████████████████████████| 839M/839M [00:06<00:00, 135MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI\n",
      "To: /home/jovyan/workspace/MakeItTalk/examples/dump/emb.pickle\n",
      "100%|██████████████████████████████████████| 30.9M/30.9M [00:02<00:00, 13.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "! mkdir MakeItTalk/examples/dump\n",
    "! mkdir MakeItTalk/examples/ckpt\n",
    "! pip install gdown\n",
    "! gdown -O MakeItTalk/examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
    "! gdown -O MakeItTalk/examples/ckpt/ckpt_content_branch.pth https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
    "! gdown -O MakeItTalk/examples/ckpt/ckpt_speaker_branch.pth https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
    "! gdown -O MakeItTalk/examples/ckpt/ckpt_116_i2i_comb.pth https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
    "! gdown -O MakeItTalk/examples/dump/emb.pickle https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445fa3e3-000f-433e-b52d-a643331d3aca",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "adapt MakeItTalk/thirdparty/AdaptativeWingLoss/core/models.py\n",
    "You want line 5 to be `from ..core.coord_conv import CoordConvTh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd48aa2a-ae7f-4d63-b467-c14677abbe6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:07:05.506443Z",
     "iopub.status.busy": "2024-02-29T23:07:05.506034Z",
     "iopub.status.idle": "2024-02-29T23:07:05.521923Z",
     "shell.execute_reply": "2024-02-29T23:07:05.521052Z",
     "shell.execute_reply.started": "2024-02-29T23:07:05.506415Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 633\n"
     ]
    }
   ],
   "source": [
    "# You can use any picture you like but it has to be 256x256 size. Here I need to adapt the size of my \"macron\" picture.\n",
    "from PIL import Image\n",
    "\n",
    "# Define input and output image paths (replace with your actual paths)\n",
    "input_path = \"../macron_square.jpg\"\n",
    "output_path = \"../macron_square_resized.jpg\"\n",
    "\n",
    "# Define the desired size for the resized image\n",
    "new_size = 256  # Adjust the size as needed\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(input_path)\n",
    "print(image.height, image.width)\n",
    "resized_image = image.resize((new_size, new_size))\n",
    "resized_image.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2086edbc-d83d-4658-a45f-92daf07ffb50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:35:46.869415Z",
     "iopub.status.busy": "2024-02-29T22:35:46.869011Z",
     "iopub.status.idle": "2024-02-29T22:35:50.191084Z",
     "shell.execute_reply": "2024-02-29T22:35:50.190473Z",
     "shell.execute_reply.started": "2024-02-29T22:35:46.869376Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/MakeItTalk\n"
     ]
    }
   ],
   "source": [
    "%cd MakeItTalk/\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"thirdparty/AdaptiveWingLoss\")\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "import torch\n",
    "import util.utils as util\n",
    "from scipy.signal import savgol_filter\n",
    "from src.approaches.train_audio2landmark import Audio2landmark_model\n",
    "from src.approaches.train_image_translation import Image_translation_block\n",
    "from src.autovc.AutoVC_mel_Convertor_retrain_version import AutoVC_mel_Convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b5aa6ef-ba45-43d9-80bc-07462a011eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:07:25.144809Z",
     "iopub.status.busy": "2024-02-29T23:07:25.144357Z",
     "iopub.status.idle": "2024-02-29T23:07:25.149038Z",
     "shell.execute_reply": "2024-02-29T23:07:25.148269Z",
     "shell.execute_reply.started": "2024-02-29T23:07:25.144779Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_head_name = \"macron_square_resized\" #\"paint_boy\"  # the image name (with no .jpg) to animate\n",
    "ADD_NAIVE_EYE = True  # whether add naive eye blink\n",
    "CLOSE_INPUT_FACE_MOUTH = (\n",
    "    False  # if your image has an opened mouth, put this as True, else False\n",
    ")\n",
    "AMP_LIP_SHAPE_X = 2.0  # amplify the lip motion in horizontal direction\n",
    "AMP_LIP_SHAPE_Y = 2.0  # amplify the lip motion in vertical direction\n",
    "AMP_HEAD_POSE_MOTION = 0.7  # amplify the head pose motion (usually smaller than 1.0, put it to 0. for a static head pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6ce9f68-033b-4e27-a9e2-dab9785a6c86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:07:27.698680Z",
     "iopub.status.busy": "2024-02-29T23:07:27.698284Z",
     "iopub.status.idle": "2024-02-29T23:07:27.709843Z",
     "shell.execute_reply": "2024-02-29T23:07:27.709152Z",
     "shell.execute_reply.started": "2024-02-29T23:07:27.698653Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--jpg\", type=str, default=\"../{}.jpg\".format(default_head_name))\n",
    "parser.add_argument(\n",
    "    \"--close_input_face_mouth\", default=CLOSE_INPUT_FACE_MOUTH, action=\"store_true\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--load_AUTOVC_name\", type=str, default=\"examples/ckpt/ckpt_autovc.pth\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load_a2l_G_name\", type=str, default=\"examples/ckpt/ckpt_speaker_branch.pth\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load_a2l_C_name\", type=str, default=\"examples/ckpt/ckpt_content_branch.pth\"\n",
    ")  # ckpt_audio2landmark_c.pth')\n",
    "parser.add_argument(\n",
    "    \"--load_G_name\", type=str, default=\"examples/ckpt/ckpt_116_i2i_comb.pth\"\n",
    ")  # ckpt_image2image.pth') #ckpt_i2i_finetune_150.pth') #c\n",
    "\n",
    "parser.add_argument(\"--amp_lip_x\", type=float, default=AMP_LIP_SHAPE_X)\n",
    "parser.add_argument(\"--amp_lip_y\", type=float, default=AMP_LIP_SHAPE_Y)\n",
    "parser.add_argument(\"--amp_pos\", type=float, default=AMP_HEAD_POSE_MOTION)\n",
    "parser.add_argument(\n",
    "    \"--reuse_train_emb_list\", type=str, nargs=\"+\", default=[]\n",
    ")  #  ['iWeklsXc0H8']) #['45hn7-LXDX8']) #['E_kmpT-EfOg']) #'iWeklsXc0H8', '29k8RtSUjE0', '45hn7-LXDX8',\n",
    "parser.add_argument(\"--add_audio_in\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--comb_fan_awing\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--output_folder\", type=str, default=\"examples\")\n",
    "\n",
    "parser.add_argument(\"--test_end2end\", default=True, action=\"store_true\")\n",
    "parser.add_argument(\"--dump_dir\", type=str, default=\"\", help=\"\")\n",
    "parser.add_argument(\"--pos_dim\", default=7, type=int)\n",
    "parser.add_argument(\"--use_prior_net\", default=True, action=\"store_true\")\n",
    "parser.add_argument(\"--transformer_d_model\", default=32, type=int)\n",
    "parser.add_argument(\"--transformer_N\", default=2, type=int)\n",
    "parser.add_argument(\"--transformer_heads\", default=2, type=int)\n",
    "parser.add_argument(\"--spk_emb_enc_size\", default=16, type=int)\n",
    "parser.add_argument(\"--init_content_encoder\", type=str, default=\"\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"learning rate\")\n",
    "parser.add_argument(\"--reg_lr\", type=float, default=1e-6, help=\"weight decay\")\n",
    "parser.add_argument(\"--write\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--segment_batch_size\", type=int, default=1, help=\"batch size\")\n",
    "parser.add_argument(\"--emb_coef\", default=3.0, type=float)\n",
    "parser.add_argument(\"--lambda_laplacian_smooth_loss\", default=1.0, type=float)\n",
    "parser.add_argument(\"--use_11spk_only\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"-f\")\n",
    "\n",
    "opt_parser = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afed81bc-9c50-450f-b34e-b2b6cefd2def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:31:23.403273Z",
     "iopub.status.busy": "2024-02-29T21:31:23.402857Z",
     "iopub.status.idle": "2024-02-29T21:31:24.049314Z",
     "shell.execute_reply": "2024-02-29T21:31:24.048355Z",
     "shell.execute_reply.started": "2024-02-29T21:31:23.403243Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In case of network errors in the next cell,\n",
    "# manually copy file \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n",
    "# \"https://www.adrianbulat.com/downloads/python-fan/3DFAN4-4a694010b9.zip\" to /home/jovyan/.cache/torch/hub/checkpoints/3DFAN4-4a694010b9.zip\n",
    "# \"https://www.adrianbulat.com/downloads/python-fan/depth-6c4283c0e0.zip\" to /home/jovyan/.cache/torch/hub/checkpoints/depth-6c4283c0e0.zip\n",
    "# ! mv ../s3fd-619a316812.pth /home/jovyan/.cache/torch/hub/checkpoints/\n",
    "# ! mv ../3DFAN4-4a694010b9.zip /home/jovyan/.cache/torch/hub/checkpoints/\n",
    "# ! mv ../depth-6c4283c0e0.zip /home/jovyan/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca86dbe4-fa19-4803-936c-6e25cc74c5eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:07:30.134423Z",
     "iopub.status.busy": "2024-02-29T23:07:30.134024Z",
     "iopub.status.idle": "2024-02-29T23:07:37.228695Z",
     "shell.execute_reply": "2024-02-29T23:07:37.227859Z",
     "shell.execute_reply.started": "2024-02-29T23:07:30.134396Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(opt_parser.jpg)\n",
    "predictor = face_alignment.FaceAlignment(\n",
    "    face_alignment.LandmarksType.THREE_D, device=\"cuda\", flip_input=True\n",
    ")\n",
    "shapes = predictor.get_landmarks(img)\n",
    "if not shapes or len(shapes) != 1:\n",
    "    print(\"Cannot detect face landmarks. Exit.\")\n",
    "    exit(-1)\n",
    "shape_3d = shapes[0]\n",
    "\n",
    "if opt_parser.close_input_face_mouth:\n",
    "    util.close_input_face_mouth(shape_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6f56022-f516-4f01-be2c-e86f7cfc59a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:50:44.905458Z",
     "iopub.status.busy": "2024-02-29T22:50:44.905033Z",
     "iopub.status.idle": "2024-02-29T22:50:44.910430Z",
     "shell.execute_reply": "2024-02-29T22:50:44.909609Z",
     "shell.execute_reply.started": "2024-02-29T22:50:44.905428Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shape_3d[48:, 0] = (shape_3d[48:, 0] - np.mean(shape_3d[48:, 0])) * 1.05 + np.mean(shape_3d[48:, 0]) # wider lips\n",
    "# shape_3d[49:54, 1] += 0.           # thinner upper lip\n",
    "# shape_3d[55:60, 1] -= 1.           # thinner lower lip\n",
    "# shape_3d[[37,38,43,44], 1] -=2.    # larger eyes\n",
    "# shape_3d[[40,41,46,47], 1] +=2.    # larger eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d71c3d28-63a1-43fe-82fe-bfcef0639932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:07:52.342375Z",
     "iopub.status.busy": "2024-02-29T23:07:52.341958Z",
     "iopub.status.idle": "2024-02-29T23:07:52.346827Z",
     "shell.execute_reply": "2024-02-29T23:07:52.346047Z",
     "shell.execute_reply.started": "2024-02-29T23:07:52.342348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape_3d, scale, shift = util.norm_input_face(shape_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a53d1b5-d931-43aa-af0b-e2fd5cdee7f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:48:32.634946Z",
     "iopub.status.busy": "2024-02-29T22:48:32.634557Z",
     "iopub.status.idle": "2024-02-29T22:48:33.355086Z",
     "shell.execute_reply": "2024-02-29T22:48:33.354290Z",
     "shell.execute_reply.started": "2024-02-29T22:48:32.634919Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 12.3.0 (conda-forge gcc 12.3.0-3)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-libass --enable-pthreads --enable-vaapi --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/pkg-config\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "\u001b[0;35m[mp3 @ 0x559c330e6b40] \u001b[0m\u001b[0;33mEstimating duration from bitrate, this may be inaccurate\n",
      "\u001b[0mInput #0, mp3, from 'examples/reply.mp3':\n",
      "  Duration: 00:00:30.26, start: 0.000000, bitrate: 32 kb/s\n",
      "  Stream #0:0: Audio: mp3, 24000 Hz, mono, fltp, 32 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'examples/reply.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf60.16.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, mono, s16, 384 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 pcm_s16le\n",
      "\u001b[1;35m[out#0/wav @ 0x559c330e8cc0] \u001b[0mvideo:0kB audio:1419kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.005369%\n",
      "size=    1419kB time=00:00:30.24 bitrate= 384.3kbits/s speed= 559x    \n"
     ]
    }
   ],
   "source": [
    "# ! cp ../reply.mp3 examples\n",
    "# ! ffmpeg -i examples/reply.mp3 examples/reply.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "887a6226-afab-4c6f-884f-678f9bfa8abe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:07:56.368967Z",
     "iopub.status.busy": "2024-02-29T23:07:56.368588Z",
     "iopub.status.idle": "2024-02-29T23:07:57.102564Z",
     "shell.execute_reply": "2024-02-29T23:07:57.101170Z",
     "shell.execute_reply.started": "2024-02-29T23:07:56.368939Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cuda in 0.01 seconds.\n",
      "Processing audio file reply.wav\n",
      "0 out of 0 are in this portion\n",
      "Loaded the voice encoder model on cuda in 0.01 seconds.\n",
      "source shape: torch.Size([1, 1920, 80]) torch.Size([1, 256]) torch.Size([1, 256]) torch.Size([1, 1920, 257])\n",
      "converted shape: torch.Size([1, 1920, 80]) torch.Size([1, 3840])\n"
     ]
    }
   ],
   "source": [
    "au_data = []\n",
    "au_emb = []\n",
    "ains = glob.glob1(\"examples\", \"*.wav\")\n",
    "ains = [item for item in ains if item != \"tmp.wav\"]\n",
    "ains.sort()\n",
    "for ain in ains:\n",
    "    os.system(\n",
    "        \"ffmpeg -y -loglevel error -i examples/{} -ar 16000 examples/tmp.wav\".format(\n",
    "            ain\n",
    "        )\n",
    "    )\n",
    "    shutil.copyfile(\"examples/tmp.wav\", \"examples/{}\".format(ain))\n",
    "\n",
    "    # au embedding\n",
    "    from thirdparty.resemblyer_util.speaker_emb import get_spk_emb\n",
    "\n",
    "    me, ae = get_spk_emb(\"examples/{}\".format(ain))\n",
    "    au_emb.append(me.reshape(-1))\n",
    "\n",
    "    print(\"Processing audio file\", ain)\n",
    "    c = AutoVC_mel_Convertor(\"examples\")\n",
    "\n",
    "    au_data_i = c.convert_single_wav_to_autovc_input(\n",
    "        audio_filename=os.path.join(\"examples\", ain),\n",
    "        autovc_model_path=opt_parser.load_AUTOVC_name,\n",
    "    )\n",
    "    au_data += au_data_i\n",
    "if os.path.isfile(\"examples/tmp.wav\"):\n",
    "    os.remove(\"examples/tmp.wav\")\n",
    "\n",
    "# landmark fake placeholder\n",
    "fl_data = []\n",
    "rot_tran, rot_quat, anchor_t_shape = [], [], []\n",
    "for au, info in au_data:\n",
    "    au_length = au.shape[0]\n",
    "    fl = np.zeros(shape=(au_length, 68 * 3))\n",
    "    fl_data.append((fl, info))\n",
    "    rot_tran.append(np.zeros(shape=(au_length, 3, 4)))\n",
    "    rot_quat.append(np.zeros(shape=(au_length, 4)))\n",
    "    anchor_t_shape.append(np.zeros(shape=(au_length, 68 * 3)))\n",
    "\n",
    "if os.path.exists(os.path.join(\"examples\", \"dump\", \"random_val_fl.pickle\")):\n",
    "    os.remove(os.path.join(\"examples\", \"dump\", \"random_val_fl.pickle\"))\n",
    "if os.path.exists(os.path.join(\"examples\", \"dump\", \"random_val_fl_interp.pickle\")):\n",
    "    os.remove(os.path.join(\"examples\", \"dump\", \"random_val_fl_interp.pickle\"))\n",
    "if os.path.exists(os.path.join(\"examples\", \"dump\", \"random_val_au.pickle\")):\n",
    "    os.remove(os.path.join(\"examples\", \"dump\", \"random_val_au.pickle\"))\n",
    "if os.path.exists(os.path.join(\"examples\", \"dump\", \"random_val_gaze.pickle\")):\n",
    "    os.remove(os.path.join(\"examples\", \"dump\", \"random_val_gaze.pickle\"))\n",
    "\n",
    "with open(os.path.join(\"examples\", \"dump\", \"random_val_fl.pickle\"), \"wb\") as fp:\n",
    "    pickle.dump(fl_data, fp)\n",
    "with open(os.path.join(\"examples\", \"dump\", \"random_val_au.pickle\"), \"wb\") as fp:\n",
    "    pickle.dump(au_data, fp)\n",
    "with open(os.path.join(\"examples\", \"dump\", \"random_val_gaze.pickle\"), \"wb\") as fp:\n",
    "    gaze = {\n",
    "        \"rot_trans\": rot_tran,\n",
    "        \"rot_quat\": rot_quat,\n",
    "        \"anchor_t_shape\": anchor_t_shape,\n",
    "    }\n",
    "    pickle.dump(gaze, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89f2c0a4-1d20-437c-ae44-b8edabbc8acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:07:59.528096Z",
     "iopub.status.busy": "2024-02-29T23:07:59.527689Z",
     "iopub.status.idle": "2024-02-29T23:08:05.025177Z",
     "shell.execute_reply": "2024-02-29T23:08:05.024167Z",
     "shell.execute_reply.started": "2024-02-29T23:07:59.528067Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/MakeItTalk\n",
      "Run on device: cuda\n",
      "Loading Data random_val\n",
      "EVAL num videos: 1\n",
      "G: Running on cuda, total num params = 3.00M\n",
      "======== LOAD PRETRAINED FACE ID MODEL examples/ckpt/ckpt_speaker_branch.pth =========\n",
      "======== LOAD PRETRAINED FACE ID MODEL examples/ckpt/ckpt_content_branch.pth =========\n",
      "====================================\n",
      "48uYS3bHIA8\n",
      "YAZuSHvwVC0\n",
      "0yaLdVk_UyQ\n",
      "E_kmpT-EfOg\n",
      "fQR31F7L3ww\n",
      "JPMZAOGGHh8\n",
      "W6uRNCJmdtI\n",
      "2KL8PfQPmBg\n",
      "p575B7k07a8\n",
      "iUoAe2gXKE4\n",
      "HH-iOC056aQ\n",
      "S8fiWqrZEew\n",
      "ROWN2ssXek8\n",
      "irx71tYyI-Q\n",
      "me6cdZCM2FY\n",
      "OkqHtWOFliM\n",
      "OfPKHc6w2vw\n",
      "1lh57VnuaKE\n",
      "_ldiVrXgZKc\n",
      "H1Xnb_rtgqY\n",
      "45hn7-LXDX8\n",
      "bs7ZWVqAGCU\n",
      "UElg0R7fmlk\n",
      "bCs5SoifsiY\n",
      "1Lx_ZqrK1bM\n",
      "RrnL6Pcjjbw\n",
      "sRbWv2R2hxE\n",
      "wJmdE0G4sEg\n",
      "hE-4e1vEiT8\n",
      "XXbxe3fCQqg\n",
      "02HOKnTjBlQ\n",
      "wAAMEC1OsRc\n",
      "7Sk--XzX8b0\n",
      "I5Lm0Qce5kg\n",
      "qLxfiUMYgQg\n",
      "_VpqWkdcaqM\n",
      "ljIkW4uVVQY\n",
      "5m5iPZNJS6c\n",
      "J-NPsvtQ8lE\n",
      "gOrQyrbptGo\n",
      "43BiUVlNy58\n",
      "swLghyvhoqA\n",
      "X3FCAoFnmdA\n",
      "2NiCRAmwoc4\n",
      "KVUf0J2LAaA\n",
      "YtZS9hH1j24\n",
      "5fZj9Fzi5K0\n",
      "wbWKG26ebMw\n",
      "QgNlXur0wrs\n",
      "qek_5m1MRik\n",
      "rmFsUV5ICKk\n",
      "bEdGv1wixF4\n",
      "ljh5PB6Utsc\n",
      "izudwWTXuUk\n",
      "B08yOvYMF7Y\n",
      "UEmI4r5G-5Y\n",
      "Scujgl9GbHA\n",
      "sxCbrYjBsGA\n",
      "qvQC0w3y_Fo\n",
      "bXpavyiCu10\n",
      "iWeklsXc0H8\n",
      "H00oAfd_GsM\n",
      "Z7WRt--g-h4\n",
      "29k8RtSUjE0\n",
      "E0zgrhQ0QDw\n",
      "9KhvSxKE6Mc\n",
      "qLNvRwMkhik\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/reply.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 12.3.0 (conda-forge gcc 12.3.0-3)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-libass --enable-pthreads --enable-vaapi --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1705436738391/_build_env/bin/pkg-config\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'examples/tmp.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf60.16.100\n",
      "  Duration: 00:00:16.00, start: 0.000000, bitrate: 5508 kb/s\n",
      "  Stream #0:0[0x1](und): Video: mjpeg (Baseline) (mp4v / 0x7634706D), yuvj420p(pc, bt470bg/unknown/unknown), 400x400, 5506 kb/s, 62.50 fps, 62.50 tbr, 10k tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[aist#1:0/pcm_s16le @ 0x55ed6b6e6f00] Guessed Channel Layout: mono\n",
      "Input #1, wav, from 'examples/reply.wav':\n",
      "  Duration: 00:00:30.26, bitrate: 256 kb/s\n",
      "  Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55ed6b6f70c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x55ed6b6f70c0] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55ed6b6f70c0] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'examples/reply_av.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuvj420p(pc, bt470bg/unknown/unknown, progressive), 400x400, q=2-31, 62.50 fps, 16k tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 aac\n",
      "[out#0/mp4 @ 0x55ed6b6a2880] video:634kB audio:137kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 2.076368%\n",
      "frame= 1000 fps=553 q=-1.0 Lsize=     787kB time=00:00:15.95 bitrate= 403.9kbits/s speed=8.82x    \n",
      "[libx264 @ 0x55ed6b6f70c0] frame I:4     Avg QP: 8.51  size:  5019\n",
      "[libx264 @ 0x55ed6b6f70c0] frame P:270   Avg QP:24.97  size:  1265\n",
      "[libx264 @ 0x55ed6b6f70c0] frame B:726   Avg QP:32.21  size:   395\n",
      "[libx264 @ 0x55ed6b6f70c0] consecutive B-frames:  1.8%  3.2%  3.0% 92.0%\n",
      "[libx264 @ 0x55ed6b6f70c0] mb I  I16..4: 86.5%  0.3% 13.2%\n",
      "[libx264 @ 0x55ed6b6f70c0] mb P  I16..4:  0.6%  1.9%  0.1%  P16..4:  6.9%  4.5%  2.1%  0.0%  0.0%    skip:83.9%\n",
      "[libx264 @ 0x55ed6b6f70c0] mb B  I16..4:  0.2%  0.3%  0.0%  B16..8:  9.7%  2.1%  0.8%  direct: 0.3%  skip:86.6%  L0:51.8% L1:46.7% BI: 1.6%\n",
      "[libx264 @ 0x55ed6b6f70c0] 8x8 transform intra:49.4% inter:6.9%\n",
      "[libx264 @ 0x55ed6b6f70c0] coded y,uvDC,uvAC intra: 4.3% 16.1% 12.2% inter: 1.3% 3.6% 3.2%\n",
      "[libx264 @ 0x55ed6b6f70c0] i16 v,h,dc,p: 78% 19%  3%  0%\n",
      "[libx264 @ 0x55ed6b6f70c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  4%  8% 88%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x55ed6b6f70c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 40% 24%  2%  1%  2%  6%  1%  6%\n",
      "[libx264 @ 0x55ed6b6f70c0] i8c dc,h,v,p: 53% 28% 19%  0%\n",
      "[libx264 @ 0x55ed6b6f70c0] Weighted P-Frames: Y:4.1% UV:0.0%\n",
      "[libx264 @ 0x55ed6b6f70c0] ref P L0: 47.5% 15.1% 18.6% 18.0%  0.8%\n",
      "[libx264 @ 0x55ed6b6f70c0] ref B L0: 77.4% 16.5%  6.1%\n",
      "[libx264 @ 0x55ed6b6f70c0] ref B L1: 91.9%  8.1%\n",
      "[libx264 @ 0x55ed6b6f70c0] kb/s:324.02\n",
      "[aac @ 0x55ed6b700e80] Qavg: 52441.484\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
    "if(len(opt_parser.reuse_train_emb_list) == 0):\n",
    "    model.test(au_emb=au_emb)\n",
    "else:\n",
    "    model.test(au_emb=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3866605c-86ea-4375-950d-63a826173282",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T23:08:07.614578Z",
     "iopub.status.busy": "2024-02-29T23:08:07.614157Z",
     "iopub.status.idle": "2024-02-29T23:08:58.124981Z",
     "shell.execute_reply": "2024-02-29T23:08:58.124044Z",
     "shell.execute_reply.started": "2024-02-29T23:08:07.614549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x67706a6d/'mjpg' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time - only video: 43.29017472267151\n",
      "Time - ffmpeg add audio: 49.150564432144165\n",
      "finish image2image gen\n"
     ]
    }
   ],
   "source": [
    "fls = glob.glob1('examples', 'pred_fls_*.txt')\n",
    "fls.sort()\n",
    "\n",
    "for i in range(0,len(fls)):\n",
    "    fl = np.loadtxt(os.path.join('examples', fls[i])).reshape((-1, 68,3))\n",
    "    fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
    "    fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
    "\n",
    "    if (ADD_NAIVE_EYE):\n",
    "        fl = util.add_naive_eye(fl)\n",
    "\n",
    "    # additional smooth\n",
    "    fl = fl.reshape((-1, 204))\n",
    "    fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 15, 3, axis=0)\n",
    "    fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
    "    fl = fl.reshape((-1, 68, 3))\n",
    "\n",
    "    ''' STEP 6: Imag2image translation '''\n",
    "    model = Image_translation_block(opt_parser, single_test=True)\n",
    "    with torch.no_grad():\n",
    "        model.single_test(jpg=img, fls=fl, filename=fls[i], prefix=opt_parser.jpg.split('.')[0])\n",
    "        print('finish image2image gen')\n",
    "    os.remove(os.path.join('examples', fls[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
