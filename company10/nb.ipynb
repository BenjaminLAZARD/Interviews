{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58b402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant. \n",
    "We want to build the family tree of the Rougon-Macquart family based on the text of the 1st book.\n",
    "We have split the task into various steps. With that in mind, help me do each task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def split_text_into_overlapping_chunks(input_text_file_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    This splits the text into chunks of defined length with predefined overlap.\n",
    "    Splits could only happen on the closest \"\\n\" character.\n",
    "    But here I am building a bare-metal solution.\n",
    "    \"\"\"\n",
    "    _SPLIT_LENGTH = 30000  # chars\n",
    "    _SPLIT_OVERLAP = 2000  # chars\n",
    "    textified_file = input_text_file_path.read_text(encoding=\"utf-8\")\n",
    "    max_char = len(textified_file)\n",
    "    starts = [\n",
    "        x - _SPLIT_OVERLAP if x != 0 else 0 for x in range(0, max_char, _SPLIT_LENGTH)\n",
    "    ]\n",
    "    ends = [\n",
    "        x if x < max_char else max_char\n",
    "        for x in range(_SPLIT_LENGTH, max_char + _SPLIT_LENGTH, _SPLIT_LENGTH)\n",
    "    ]\n",
    "\n",
    "    # print(starts[:10], ends[:10], max_char)\n",
    "    # return\n",
    "\n",
    "    (input_text_file_path.parent / \"splits\").mkdir(exist_ok=True)\n",
    "    for i in range(len(starts)):\n",
    "        chunk_file_path = input_text_file_path.parent / f\"\"\"splits/{i}/text.txt\"\"\"\n",
    "        chunk_file_path.parent.mkdir(exist_ok=True)\n",
    "        chunk_text = textified_file[starts[i] : ends[i]]\n",
    "        chunk_file_path.write_text(chunk_text, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "split_text_into_overlapping_chunks(Path(\"data/La_Fortune_des_Rougon.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d88439",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Travail\\Preparation entretiens\\code_tests\\company10\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1014\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Travail\\Preparation entretiens\\code_tests\\company10\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m         extracted_name_file = p / \u001b[33m\"\u001b[39m\u001b[33mextracted_names.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m         extracted_name_file.write_text(response.choices[\u001b[32m0\u001b[39m].message.content, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mlist_character_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/splits\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mlist_character_names\u001b[39m\u001b[34m(split_directory)\u001b[39m\n\u001b[32m     40\u001b[39m chunk_text = chunk_file.read_text(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m messages = [\n\u001b[32m     42\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[32m     43\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt.format(chunk_text=chunk_text)}\n\u001b[32m     44\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m extracted_name_file = p / \u001b[33m\"\u001b[39m\u001b[33mextracted_names.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m extracted_name_file.write_text(response.choices[\u001b[32m0\u001b[39m].message.content, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Travail\\Preparation entretiens\\code_tests\\company10\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Travail\\Preparation entretiens\\code_tests\\company10\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Travail\\Preparation entretiens\\code_tests\\company10\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Travail\\Preparation entretiens\\code_tests\\company10\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1019\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Travail\\Preparation entretiens\\code_tests\\company10\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1060\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1057\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1058\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "def list_character_names(split_directory: Path):\n",
    "    \"\"\"\n",
    "    List all characters that are mentionned in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    At this stage, we want to build a list of all characters that are mentionned in the text.\n",
    "\n",
    "    The output will be a text file (do not include any other information).\n",
    "\n",
    "    Bear in mind the following guidelines:\n",
    "\n",
    "    - List all character names, with known nicknames. For example: \"Pierre Rougon -> [Pierre, le capitaine,  Pierrot, etc.]\". Do not include references that could refer to several characters (there could be several M. Rougon). We will assume that there is only one character per typle (firstname, lastname)\n",
    "    - If there are various surnames known in the paragraph, list them all. For example: \"Pierre Rougon -> [Pierre, le capitaine,  Pierrot, etc.]\n",
    "    - There should be one line per character (and one character per line)\n",
    "    - The character name put first should be the most relevant (ideally \"firstname lastname\") if this is not known in the pragraph, then use the most relevant nickname.\n",
    "    - If the member is a Rougon family members, prefix with *. A Rougon-Macquart may not bear the familly name and yet be related (for example through wedding)\n",
    "    - Do not invent nickanmes or assume a character is a Rougon-Macquart if there is no eveidence of it in the text.\n",
    "    - Do not include the full name to the left of the arrow in the list of names. If there is no known other name, just leave the list empty.\n",
    "    - Do not put family relationships in the list of names except if this is the only reference to the character. for example \"grand-père\" could be a nickname but if his name is Jean then it should not write \"Jean ->[grand-père de Jacques]\"\n",
    "\n",
    "\n",
    "    Example expected output:\n",
    "    ```txt\n",
    "    * Pierre Rougon -> [Pierre, Pierrot, le marchand d'huile, etc.]\n",
    "    * François Mouret -> [etc.]\n",
    "    Boutigny -> [etc.]\n",
    "    ```\n",
    "    \n",
    "    ############################################################\n",
    "    Provided text:\n",
    "    ############################################################\n",
    "    {chunk_text}\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    for p in split_directory.iterdir():\n",
    "        if p.is_file():\n",
    "            continue\n",
    "        chunk_file = p / \"text.txt\"\n",
    "        chunk_text = chunk_file.read_text(encoding=\"utf-8\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt.format(chunk_text=chunk_text)}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        extracted_name_file = p / \"extracted_names.txt\"\n",
    "        extracted_name_file.write_text(response.choices[0].message.content, encoding=\"utf-8\")\n",
    "\n",
    "list_character_names(Path(\"data/splits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfca2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "def consolidate_character_names(split_directory: Path):\n",
    "    \"\"\"\n",
    "    Consolidate the character names from all splits into a single file.\n",
    "\n",
    "    NB Given we only have 23 splits here, I am merging all extracted names in one file.\n",
    "    If we were to build a truly scalable solution, we would need some sort of map-redude:\n",
    "    take the first 2 names\n",
    "    \"\"\"\n",
    "    name_list = \"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    At this stage, we have a split the main text into smaller chunks.\n",
    "\n",
    "    For each chunk, we have built a list of names as follows:\n",
    "    \"known name -> [nickname1, nickname2, etc.]\"\n",
    "    Ideally the known name is the character's full name (firstname lastname), but it can also be a nickname (like \"tante Dide\")\n",
    "\n",
    "    We want to consolidate all the lists of names (each being built on a separate chunk) into a single list of names.\n",
    "\n",
    "    The output format should be similar to individual lists of names that is:\n",
    "    ```txt\n",
    "    * Pierre Rougon -> [Pierre, Pierrot, le marchand d'huile, etc.]\n",
    "    * François Mouret -> [etc.]\n",
    "    Boutigny -> [etc.]\n",
    "    ```\n",
    "\n",
    "    The criteria for the merge are as follows:\n",
    "    - There should be one line per character and one line only.\n",
    "        - If 2 lines or more refer to the same character, keep the most accurate designation (ideally full name) as name and include in the list the merge of all nicknames from the combined lines\n",
    "        - If 2 lines refer to the same character with different names, merge the 2 lines either way following the directive above.\n",
    "        - If 1 line in the merge has an asterisk, keep it in the merge as a prefix.\n",
    "    - If the member is a Rougon family members, prefix with *. A Rougon-Macquart may not bear the familly name and yet be related (for example through wedding\n",
    "    - Do not include the full name to the left of the arrow in the list of names. If there is no known other name, just leave the list empty.\n",
    "    - If a name is too vague and can refer to various characters without a clear explanation, and that the list does not help with the diambiguation, then drop it. But if the name is actually the family name of a specific character precised in the list, then use it in the merge\n",
    "\n",
    "    Example expected output:\n",
    "    ```txt\n",
    "    * Pierre Rougon -> [Pierre, Pierrot, le marchand d'huile, etc.]\n",
    "    * François Mouret -> [etc.]\n",
    "    Boutigny -> [etc.]\n",
    "    ```\n",
    "\n",
    "    Do not include any other information in the output (no explanation or header)\n",
    "\n",
    "    The lists of names per chunk are separated by a line jump    \n",
    "    ############################################################\n",
    "    Provided lists of names:\n",
    "    ############################################################\n",
    "    {name_list}\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    for p in split_directory.iterdir():\n",
    "        if p.is_file():\n",
    "            continue\n",
    "        names_file = p / \"extracted_names.txt\"\n",
    "        name_list += (\"\\n\" + names_file.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(name_list=name_list)}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    consolidated_names_file = split_directory / \"consolidated_names.txt\"\n",
    "    consolidated_names_file.write_text(response.choices[0].message.content, encoding=\"utf-8\")\n",
    "\n",
    "consolidate_character_names(Path(\"data/splits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a64a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "def desambiguate_characters(split_directory: Path):\n",
    "    \"\"\"\n",
    "    List clearly the characters at hand in the text (disambiguate pronouns and nicknames).\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    At this stage, we have a list of characters that are mentionned in the text and we have identified those that are Rougon-Macquart.\n",
    "\n",
    "    For further extraction of relationships between characters in a later stage, we want to rewrite an excerpt of the book with the appropriate name as identified per the list.\n",
    "    We want to replace any character reference (nickname, pronoun, etc.) with the character identified in the list between brackets.\n",
    "    We want to  perform this change only for members of the Rougon-Macquart family identified by the asterisk (*) that prefixes their name in the list.\n",
    "\n",
    "    ##### EXAMPLE LIST OF NAMES #####\n",
    "\n",
    "    * Pierre Rougon -> [Pierre, le marchand d'huile, le jeune Rougon, le mari]\n",
    "    * Félicité Rougon -> [Félicité, sa femme, la petite, la vieille gueuse, la petite mère]\n",
    "    Chantegreil -> [le braconnier Chantegreil, le père de Miette]\n",
    "    Eulalie Chantegreil -> [Eulalie, la tante Eulalie]\n",
    "\n",
    "    ##### EXAMPLE TEXT INPUT #####\n",
    "\n",
    "    À ce moment, on vit circuler Aristide parmi les groupes. Le cher garçon, devant ce soulèvement formidable, avait pensé qu’il était imprudent de ne pas rester l’ami des républicains ; mais comme, d’un autre côté, il ne voulait pas trop se compromettre avec eux, il était venu leur faire ses adieux, le bras en écharpe, en se plaignant amèrement de cette maudite blessure qui l’empêchait de tenir une arme.\n",
    "    Alors Pierre s’arrêta un instant sur le trottoir désert. Il poussa un gros soupir de soulagement et de triomphe. Ces gueux de républicains lui abandonnaient donc Plassans. La ville lui appartenait, à cette heure : elle dormait comme une sotte ; elle était là, noire et paisible, muette et confiante, et il n’avait qu’à étendre la main pour la prendre. Cette courte halte, ce regard d’homme supérieur jeté sur le sommeil de toute une sous-préfecture, lui causèrent des jouissances ineffables. Il resta là, croisant les bras, prenant, seul dans la nuit, une pose de grand capitaine à la veille d’une victoire. Au loin, il n’entendait que le chant des fontaines du cours, dont les filets d’eau sonores tombaient dans les bassins.\n",
    "\n",
    "    ##### EXAMPLE OUTPUT #####\n",
    "\n",
    "    À ce moment, on vit circuler Aristide parmi les groupes. Le cher garçon, devant ce soulèvement formidable, avait pensé qu’il était imprudent de ne pas rester l’ami des républicains ; mais comme, d’un autre côté, il ne voulait pas trop se compromettre avec eux, il était venu leur faire ses adieux, le bras en écharpe, en se plaignant amèrement de cette maudite blessure qui l’empêchait de tenir une arme.\n",
    "    Alors [Pierre Rougon] s’arrêta un instant sur le trottoir désert. Il poussa un gros soupir de soulagement et de triomphe. Ces gueux de républicains lui ([Pierre Rougon]) abandonnaient donc Plassans. La ville lui ([Pierre Rougon]) appartenait, à cette heure : elle dormait comme une sotte ; elle était là, noire et paisible, muette et confiante, et il ([Pierre Rougon]) n’avait qu’à étendre la main pour la prendre. Cette courte halte, ce regard d’homme supérieur jeté sur le sommeil de toute une sous-préfecture, lui ([Pierre Rougon]) causèrent des jouissances ineffables. Il([Pierre Rougon]) resta là, croisant les bras, prenant, seul dans la nuit, une pose de grand capitaine à la veille d’une victoire. Au loin, il ([Pierre Rougon]) n’entendait que le chant des fontaines du cours, dont les filets d’eau sonores tombaient dans les bassins.\n",
    "\n",
    "    \n",
    "    ############################################################\n",
    "    Provided list of names:\n",
    "    ############################################################\n",
    "    {names_list}\n",
    "    ############################################################\n",
    "    Provided intput text:\n",
    "    ############################################################\n",
    "    {input_text}\n",
    "\n",
    "    ############################################################\n",
    "    Expected output text (complete here)\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    consolidated_name_list = (split_directory / \"consolidated_names.txt\").read_text(encoding=\"utf-8\")\n",
    "\n",
    "    for p in split_directory.iterdir():\n",
    "        if p.is_file():\n",
    "            continue\n",
    "        chunk_file = p / \"text.txt\"\n",
    "        chunk_text = chunk_file.read_text(encoding=\"utf-8\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt.format(names_list=consolidated_name_list, input_text=chunk_text)}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        desambiguated_text_file = p / \"desambiguated_text.txt\"\n",
    "        desambiguated_text_file.write_text(response.choices[0].message.content, encoding=\"utf-8\")\n",
    "\n",
    "desambiguate_characters(Path(\"data/splits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab995b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "def find_relationships(split_directory: Path):\n",
    "    \"\"\"\n",
    "    List the relationships between the members of the Rougon-Macquart family based on the text (per chunk).\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    At this stage, we have a list of characters that are mentionned in the text and we have identified those that are Rougon-Macquart. (an asterisk (*) prefixes their name in the list).\n",
    "    We also have text chunks with clear mentions of characters that are members of this family (they are written between brackets).\n",
    "\n",
    "    We want to start extrating the relationships between the members of the Rougon-Macquart family based on the text.\n",
    "\n",
    "    We will provide the list of names and a chunk of the book as input.\n",
    "\n",
    "    You need to output a list of relationships between the members of the Rougon-Macquart family (only those) as follows:\n",
    "\n",
    "    Example expected output:\n",
    "    ```txt\n",
    "    * A -> B [married]\n",
    "    * C -> D [parent]\n",
    "    * A -> C [parent]\n",
    "    ```\n",
    "\n",
    "    Where A,B,C,D are the names of the characters as they appear in the list of names (just the main full name, not the nickname). The relationship is written between brackets.\n",
    "\n",
    "    The goal will be to create a family tree later. Consequently we will build a DAG (using DOT language most likely)\n",
    "    The rules for this are as follows:\n",
    "    - Between 2 characters there should only be one relationship. If there is a choice to make, we pick downards when possible (i.ie parent insteado of child)\n",
    "    - Choose direct relationships and do not add exta relationships (example if A is parent of B and B is parent of C, do not add A -> C [grandparent]. Seeminlingly if grandparent relationship is mentioned in the text, try to break it down into 2 relationships (A -> B [parent] and B -> C [parent]).\n",
    "    - When possible try to write only \"parent\" and \"married\" relationships (no mother/father/son/sibling/ nephew or whatever whenever it can be avoided). Should some members of the familly tree be missing, add them as \"unknown\" (example: A -> unknwon1 [parent] and unknown1 -> C [parent]). Be careful to not mix up various unknwonws (always add the right numberas a suffix to refer to the appropriate one).\n",
    "    - if a non direct relationship is indicated in the text, then add it as is.\n",
    "\n",
    "\n",
    "    \n",
    "    ############################################################\n",
    "    Provided list of names:\n",
    "    ############################################################\n",
    "    {names_list}\n",
    "\n",
    "    ############################################################\n",
    "    Provided intput text:\n",
    "    ############################################################\n",
    "    {input_text}\n",
    "\n",
    "    ############################################################\n",
    "    Expected relationships:  (complete here)\n",
    "    ############################################################\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    consolidated_name_list = (split_directory / \"consolidated_names.txt\").read_text(encoding=\"utf-8\")\n",
    "\n",
    "    for p in split_directory.iterdir():\n",
    "        if p.is_file():\n",
    "            continue\n",
    "        chunk_file = p / \"desambiguated_text.txt\"\n",
    "        chunk_text = chunk_file.read_text(encoding=\"utf-8\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt.format(names_list=consolidated_name_list, input_text=chunk_text)}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        relationships_file = p / \"relationships.txt\"\n",
    "        relationships_file.write_text(response.choices[0].message.content, encoding=\"utf-8\")\n",
    "\n",
    "find_relationships(Path(\"data/splits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c235883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "def consolidate_relationships(split_directory: Path):\n",
    "    \"\"\"\n",
    "    Consolidate the relationships from all splits into a single file.\n",
    "\n",
    "    We have now extracted for several chunks of text from the book the relationships between the characters.\n",
    "    Based upon them, we want to build a single list of relationships.\n",
    "    \"\"\"\n",
    "    relationships_list = \"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    At this stage, we have now extracted for several chunks of text from the book the relationships between the characters.\n",
    "    Based upon them, we want to consolidate results into a single list of relationships.\n",
    "\n",
    "    The output format should be similar to individual lists of relationships that is:\n",
    "    ```txt\n",
    "    * A -> B [married]\n",
    "    * C -> D [parent]\n",
    "    * A -> C [parent]\n",
    "    ```\n",
    "\n",
    "    The criteria for the merge are as follows:\n",
    "    - Try to use only \"parent\" and \"married\" relationships whenever possible\n",
    "    - If there are some \"holes\" in the family tree relationships, add them as \"unknown\" (example: A -> unknwon1 [parent] and unknown1 -> C [parent]). Be careful to not mix up various unknwonws (always add the right numberas a suffix to refer to the appropriate one).\n",
    "    - There should be one line per relationship maximum.\n",
    "        - If 2 written family ties or more indicate the same relationship, then keep only downards relationships (i.e. parent instead of child)\n",
    "    - The relationships in the end should be akin to a DAG (there should not be cycles, or duplicate nodes).\n",
    "\n",
    "    If there happen to be incompatibilities (clear contract)\n",
    "\n",
    "\n",
    "    Do not include any other information in the output\n",
    "\n",
    "    The lists of relationships found per chunk are separated by a line jump    \n",
    "    \n",
    "    ############################################################\n",
    "    Provided list of relationships (lists are separated by line jumps):\n",
    "    ############################################################\n",
    "    {relationships_lists}\n",
    "\n",
    "    ############################################################\n",
    "    Expected relationships (complete here without comments or additional text or header)\n",
    "    ############################################################\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for p in split_directory.iterdir():\n",
    "        if p.is_file():\n",
    "            continue\n",
    "        relationship_file = p / \"relationships.txt\"\n",
    "        relationships_list += (\"\\n\" + relationship_file.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(relationships_lists=relationships_list)}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    consolidated_names_file = split_directory / \"consolidated_relationships.txt\"\n",
    "    consolidated_names_file.write_text(response.choices[0].message.content, encoding=\"utf-8\")\n",
    "\n",
    "consolidate_relationships(Path(\"data/splits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a28c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7\n",
    "def build_graph(split_directory: Path):\n",
    "    \"\"\"\n",
    "    Build a Graphviz compatible graph based on a list of relationships.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    At this stage, we have built a list of relationships between the members of the Rougon-Macquart family.\n",
    "    We now want to build a graphviz compatible graph (.DOT language) based on these relationships.\n",
    "\n",
    "    The goal is to reconstruct the family tree of the Rougon-Macquart family.\n",
    "    Consequently the final output should respect the following rules:\n",
    "\n",
    "    - The graph should be a DAG with no loops\n",
    "    - There should only be direct relationships (i.e. no grandparent, uncle, etc.) : only \"parent\" and \"married\" relationships should be used.\n",
    "    - If need be \"unknown_i\" can be created to fill in the gaps in the family tree when the name of an intermediary parent is not known.\n",
    "    - The arrows should NOT be labeled with the type of relationship (i.e. \"parent\" or \"married\"), and the nodes be the name of the characters.\n",
    "    - Each character should be represented by a single node (i.e. no duplicates).\n",
    "\n",
    "    - \"married\" relationship representing married couples should be grouped with an intermediary node \"m1 [label = \"{{A Rougon | B Rougon}}\";];\" and then the corresponding node has the children: \"m1 -> \"D Rougon\"; m1 -> \"E Rougon\"; So there should not be individual nodes for each member of the married couple.\n",
    "    - \"parent\" relationshipis the only relationships between the intermediary nodes of couples and the children. This is represented by the standard arrow (without label)\n",
    "\n",
    "    ############################################################\n",
    "    Provided lists of relationships:\n",
    "    ############################################################\n",
    "    {relationships_lists}\n",
    "    \"\"\"\n",
    "\n",
    "    consolidated_relationships_file  = split_directory / \"consolidated_relationships.txt\"\n",
    "\n",
    "    relationships_list = consolidated_relationships_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(relationships_lists=relationships_list)}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    graphviz_output_file = split_directory / \"graphviz_output.dot\"\n",
    "    graphviz_output_file.write_text(response.choices[0].message.content, encoding=\"utf-8\")\n",
    "\n",
    "build_graph(Path(\"data/splits\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
